# Aurora Motors Validation Suite - WITH CHUNKING ENABLED
#
# This is an example showing how to enable streaming/chunked validation
# for large datasets. When enabled, data is fetched and validated in chunks
# rather than loading the entire dataset into memory.
#
# Benefits:
# - Can handle datasets of any size (millions of rows)
# - Validates while data is loading (overlapped I/O)
# - Concurrent chunk processing for faster throughput
# - Adaptive chunk sizing learns optimal settings over time

metadata:
  suite_name: "Aurora_Motors_Validation_Chunked"
  index_column: "MATERIAL_NUMBER"
  description: "Aurora Motors material validation with streaming chunked processing"
  data_source: "get_aurora_motor_dataframe"  # Query function from core/queries.py

  # Chunking configuration (optional - omit entirely to use intelligent defaults)
  # Chunking is ALWAYS ENABLED and auto-tunes based on dataset size:
  # - Small datasets (< 50k rows): Processed as single batch
  # - Large datasets: Automatically chunked with optimal settings
  #
  # Override defaults here if needed:
  chunking:
    target_time_seconds: 300         # Target: complete 1M rows in 5 minutes (default: 300)
    initial_chunk_size: 75000        # Start with 75k rows per chunk (default: 75000)
    max_workers: 4                   # Process 4 chunks concurrently (default: 4)
    min_rows_for_chunking: 50000     # Don't chunk below this threshold (default: 50000)

  # How auto-chunking works:
  # 1. Data is fetched in chunks (e.g., 75k rows at a time) using LIMIT/OFFSET
  # 2. Each chunk is validated independently using GX expectations
  # 3. Multiple chunks are processed concurrently (max_workers=4)
  # 4. Results are merged automatically after all chunks complete
  # 5. Performance metrics are tracked and chunk size adapts over time

validations:
  # --- Required fields (null checks) ---
  - type: "expect_column_values_to_not_be_null"
    columns:
      - "GROSS_WEIGHT"
      - "NET_WEIGHT"
      - "PACK_INDICATOR"
      - "PRICING_GROUP"
      - "PRICE_GROUP_CODE"
      - "Z01_MKT_MTART"
      - "MATERIAL_GROUP_4"
      - "ITEM_CATEGORY_GROUP"

  # --- Fixed value expectations ---
  - type: "expect_column_values_to_be_in_set"
    rules:
      PRICING_GROUP: ["AM"]
      PLANT: ["00A"]
      SALES_ORGANIZATION: ["BEC"]
      STORAGE_LOCATION: ["0001"]
      STORAGE_TYPE: ["001"]
      Z01_MKT_MTART: ["CAT"]
      ITEM_CATEGORY_GROUP: ["NORM"]
      SALES_ITEM_CATEGORY_GROUP: ["NORM"]
      DELIVERING_PLANT: ["00A"]
      LOADING_GROUP: ["0002"]
      ACCOUNT_ASSIGNMENT_GROUP: ["01"]
      AVAILABILITY_CHECK: ["02"]
      PRICING_REFERENCE_MATERIAL: ["3"]
      MATERIAL_PRICING_GROUP: ["01"]
      MATERIAL_STATISTICS_GROUP: ["1"]
      GENERAL_ITEM_CATEGORY_GROUP: ["NORM"]
      TRANSPORTATION_GROUP: ["0002"]
      MRP_TYPE: ["ND"]
      PROCUREMENT_TYPE: ["F"]
      AVAILABILITY_CHECK_MRP: ["02"]
      COUNTRY_OF_ORIGIN: ["US"]
      ABC_INDICATOR: ["C"]
      MRP_GROUP: ["B01"]
      PRODUCT_HIERARCHY: ["522475549547036171"]
      DISTRIBUTION_CHANNEL: ["10"]
      WAREHOUSE_NUMBER: ["M01"]

  # --- Regex pattern matching ---
  - type: "expect_column_values_to_match_regex"
    columns:
      - "PACK_INDICATOR"
    regex: '^\s*$'  # Must be whitespace only

  # --- Column comparisons ---
  - type: "expect_column_pair_values_a_to_be_greater_than_b"
    column_a: "GROSS_WEIGHT"
    column_b: "NET_WEIGHT"
    or_equal: true  # GROSS_WEIGHT >= NET_WEIGHT

  - type: "expect_column_pair_values_to_be_equal"
    column_a: "ITEM_CATEGORY_GROUP"
    column_b: "SALES_ITEM_CATEGORY_GROUP"

# Performance Notes:
# - Initial run will use initial_chunk_size (75k rows)
# - Subsequent runs will automatically adjust chunk size based on:
#   * Average throughput (rows/second)
#   * Target completion time (300 seconds)
#   * Number of concurrent workers (4)
# - Performance history is saved in: validation_results/performance/
# - You can view performance stats by checking the JSON files in that directory
